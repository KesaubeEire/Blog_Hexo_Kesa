---
title: DS|统计学习方法(一)
date: 2019-03-16 19:42:57
tags: ML
categories: Data Science
comments: false     # 禁用评论，可选，默认开启
reward: true        # 禁用打赏，可选，默认开启
---

拜读先贤的名著
<!-- more -->

# 第一章 : 概论

## 统计学习

- 特点   : 以方法为中心,以交叉学科为特征,以数据为驱动...
- 对象   : 数据
- 目的   : 预测\分析
- 方法   : 监督\非监督\半监督\强化
- 研究   : 方法\理论\应用
- 重要性 : 海量数据|智能化|科学发展的必然

## 监督学习

### 基本概念

监督学习（supervised learning）的任务是学习一个模型，  
使模型能够对任意给定的输入，对其相应的输出做出一一个好的预测  
（注意，这里的输入、输出是指某个系统的输入与输出，与学习的输入与输出不同）。  

- 输入空间\特征空间\输出空间
    - 问题分类:
        - 输入变量与输出变量**均为连续变量**的预测问题称为回归问题；
        - 输出变量为**有限个离散变量**的预测问题称为分类问题；
        - 输入变量与输出变量**均为变量序列**的预测问题称为标注问题。

- 联合概率分布

监督学习假设输入与输出的随机变量 X 和 Y 遵循联合概率分布 P (X, Y).  
P (X, Y）表示分布函数，或分布密度函数。  
注意,在学习过程中，假定这一联合概率分布存在，  
但对学习系统来说，联合概率分布的具体定义是未知的。  
训练数据与测试数据被看作是依联合概率分布 P (X, Y）独立同分布产生的。  
统计学习假设数据存在一定的统计规律，X 和 Y 具有联合概率分布的假设就是监督学习关于数据的基本假设。

- 假设空间

监督学习的目的在于学习-一个由输入到输出的映射，  
这一映射由模型来表示。换句话说，学习的目的就在于找到最好的这样的模型。  
模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间（hypothesis space）。  
假设空间的确定意味着学习范围的确定。

### 问题的形式化

![监督学习问题.jpg](https://kesa-hexo-1256284069.cos.ap-beijing.myqcloud.com/2019/03/17/22db5b5a63cb0d52adcf7324e2489d5e.jpg)

## 统计学习三要素

- 模型
- 策略
  - 损失|风险函数
    - 零一:太绝对,只在分类中出现?
    - 平方:出现较多
    - 绝对
    - 对数:极大似然估计用对数估计 -> 逻辑回归的过程就是这个
      - 相乘的概率越来越小,这样让信息得以保留,让乘法有意义
      - 减小计算量
  - 经验风险最小化|结构风险最小化函数
    - 经验:损失量取平均 适用于大数据的情况
    - 结构:有个λ - 0.1 1 10 100 ….一个惩罚项的系数,防止过拟合
- 具体算法

## 模型评估与模型选择

- 误差
- 过拟合

## 正则化

- L1范数L2范数的不同
- λ系数的取值范围

# 交叉验证

- 普通验证:分三类 [训练 | 验证 | 测试]
- 交叉验证
  - 比例
  - 分类
    - 简单
    - s
      - 一般是7:3的比例
    - 留一
  - 意义:对所有数据的都来一下

## 泛化能力

- 泛化误差：损失函数的期望->象征着一种平均；

## 生成模型与判别模型

- 生成方法:由数据学习联合概率分布，然后求出条件概率分布；收敛速度更快；

- 判别方法: 由数据直接学习决策函数或者条件概率分布；

## 分类问题

- 分类问题:分类器，输入序列和标记训练，输入新序列输出标记；
- 分类性能指标:
  - 指标
    - TP：实际为正、且划分为正的样本数，真正数。 
    - FP：实际为负、但划分为正的样本数，假正数。 
    - TP：实际为负、且划分为负的样本数，真负数。 
    - FN：实际为正、但划分为负的样本数，假负数。
  - 正确率（Accuracy）：预测正确的结果所占总数的比例。 
    -  $$\ Accuracy=\frac{TP+TN}{TP+TN+FP+FN} $$
    - 正确率高并不能说明分类器效果好。比如识别是否一个人是否有艾滋病，就算全部预测为负样本，那正确率也有99%
      以上（因为每万人才有6个艾滋病人），但是这种高正确率的分类器其实并没有任何作用。
  - 精确率（Precision）：预测为正且预测正确的样本占所有预测为正的样本的比例，又叫做查准率。 
    - $$Precision=\frac{TP}{TP+FP} $$
    - 精确率高意味着，只要识别出来是正的，就肯定是正的。侧重将不易区分的样本划分为负样本。考察的是识别出来的正样品是否靠谱。适用于 宁缺毋滥 的场景，比如识别一个面试者是否符合录取条件。
  - 召回率（Recall,Sensitivity）：预测为正且预测正确的样本占所有实际为正的样本的比例，又叫敏感度、查全率。 
    - $$ Recall= \frac{TP}{TP+FN} $$
    - 召回率高意味着，只要是正的，都能识别出来。侧重将不易区分的样本划分为正样本。考察的是对正样品是否敏感。适用于 宁可错杀一百不可放过一人 的场景，比如识别是否是犯罪嫌疑人、非典期间识别是否可能患了非典。
  - 特异性（Specificity）：预测为负且预测正确的样本占所有实际为负的样本的比例。 
    - $$ Specificity=\frac {TN}{TN+FP}$$
    - 特异性相当于负样本的“召回率”。 
      特异性高意味着，只要是负的，都能识别出来。侧重将不易区分的样本划分为负样本。考察的是对负样本是否敏感。

## 标注问题

- 输入：观测序列；输出：标记序列和状态序列；
- 思想：条件概率分布；
  对一个观测序列X找到使得条件概率P(X|Y)最大的标记序列Y；

## 回归问题

- 预测
  常用损失函数:平方损失函数等；

# 第二章:感知机


## 概念

感知机是根据输入实例的特征向量 x 对其进行二类分类的线性分类模型：
$ f(x) = sign(wx +b)​$

感知机模型对应于输入空间（特征空间）中的分离超平面 $ wx+b = 0​$.



## 学习策略

感知机学习的策略是极小化损失函数：

损失函数对应于误分类点到分离超平面的总距离,由误分类驱动.

## 学习算法

感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，  
有原始形式和对偶形式。  
算法简单且易于实现。  
原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。  
在这个过程中一次随机选取一个误分类点使其梯度下降。  
对偶形式也是收敛的.

## 代码:

class Model:
    
        def __init__(self):
            self.w = np.ones(len(data[0])-1, dtype=np.float32)
            self.b = 0
            self.l_rate = 0.1
            # self.data = data
        
        def sign(self, x, w, b):
            y = np.dot(x, w) + b
            return y
        
        # 随机梯度下降法
        def fit(self, X_train, y_train):
            is_wrong = False
            while not is_wrong:
                wrong_count = 0
                for d in range(len(X_train)):
                    X = X_train[d]
                    y = y_train[d]
                    if y * self.sign(X, self.w, self.b) <= 0:
                        self.w = self.w + self.l_rate*np.dot(y, X)
                        self.b = self.b + self.l_rate*y
                        wrong_count += 1
                if wrong_count == 0:
                    is_wrong = True
            return 'Perceptron Model!'
            
        def score(self):
            pass

### 延伸

- 口袋算法
- 表决算法
- 带边缘的感知机